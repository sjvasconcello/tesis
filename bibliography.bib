% Intro
@article{intro1,
   abstract = {The aim of this paper is to reassess the current view of technological trends adopting a historical perspective. In our interpretation, the historical record provides some suggestive evidence for a more sceptical view of the notion of an emerging “fourth” industrial revolution. Indeed, even at an impressionistic glance, the recent developments in AI, communication and robotics that are marked as the core of the fourth industrial revolution, appear as a rather natural prolongation of the ICT macro-trajectories described in this paper. At the same time, to study the relation between technology and labour, we focus on the plant level as the most useful unit of analysis to consider the complex interaction between management systems, labour process and technological innovations. In this sense, we examine two Internet of Things’ technologies in order to underline the persistence of a fundamental trait of the capitalist mode of production, namely the exertion of control over workers. Consistently, we expect a continuity between newly emerging management practices and previous management systems, especially referring to the ones adopted during the ICT revolution.},
   author = {Armanda Cetrulo and Alessandro Nuvolari},
   doi = {10.1007/S40812-019-00132-Y/TABLES/1},
   issn = {19724977},
   issue = {3},
   journal = {Journal of Industrial and Business Economics},
   keywords = {Control,ICT revolution,Industry 4.0,Management system},
   month = {9},
   pages = {391-402},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Industry 4.0: revolution or hype? Reassessing recent technological trends and their impact on labour},
   volume = {46},
   url = {https://link.springer.com/article/10.1007/s40812-019-00132-y},
   year = {2019},
}

@article{intro2,
   abstract = {In March 2023, the release of GPT-4 and its application, Copilot, astounds the world and thrusts AI into the spotlight in industry, and academia. The incredible superiority of GPT-4 is demonstrated by its ability to achieve high scores on almost all mainstream academic and professional standard exams, Copilot’s capability to accomplish nearly all repetitive office work, and the rapid spread of its applications across massive areas of human society within weeks of its launch. These changes lead to the belief on the emergence of GPT-4 having a significant impact on academic research in the finance and accounting fields by establishing a consensus on the psychological acceptance of AI and rapidly eliminating technical barriers of using it. This paper presents practical examples to demonstrate GPT-4’s effectiveness in sentiment analysis, ESG analysis, corporate culture analysis, and Federal Reserve opinion analysis, and provides instructive recommendations for applying it in these subject areas.},
   author = {Yi Cao and Jia Zhai},
   doi = {10.1080/14765284.2023.2212434},
   issn = {14765292},
   issue = {2},
   journal = {Journal of Chinese Economic and Business Studies},
   pages = {177-191},
   publisher = {Routledge},
   title = {Bridging the gap–the impact of ChatGPT on financial research},
   volume = {21},
   url = {https://www.tandfonline.com/action/journalInformation?journalCode=rcea20},
   year = {2023},
}

@article{intro3,
   abstract = {Conversational AI models like ChatGPT, developed by OpenAI, are a testament to the rapid advancements in artificial intelligence and language processing capabilities. ChatGPT is a language model trained on massive amounts of data, capable of performing a range of language-related tasks, including answering questions, generating text, and even writing poems. Its impressive performance has attracted significant attention from both researchers and industry professionals, leading to its widespread use in a variety of applications. However, the deployment of conversational AI models like ChatGPT raises important ethical and social considerations. There are concerns about the potential for AI systems to perpetuate biases and stereotypes, and the impact they may have on employment. As such, it is crucial that the development and deployment of these models be guided by ethical considerations and principles. The potential benefits of conversational AI models like ChatGPT are significant and far-reaching. In the customer service sector, for example, they can provide 24/7 support and improve the overall customer experience. In the content creation and marketing industries, they can be used to generate high-quality content, freeing up human workers to focus on more creative and strategic tasks. Additionally, they have the potential to revolutionize the way we interact with technology, changing the way we communicate and access information. ChatGPT represents a significant breakthrough in the field of AI language processing, with the potential to transform various industries and improve our lives. However, it is essential that we approach its development and deployment with caution, taking into account the potential ethical and social implications. The continued growth and evolution of conversational AI models like ChatGPT will shape the future of human-computer interaction, and it is up to us to actively monitor and mitigate any adverse consequences. In this research we will be discussing about all of these topics.},
   author = {Puranjay Savar Mattas},
   doi = {10.55248/GENGPI.2023.4218},
   issue = {02},
   journal = {International Journal of Research Publication and Reviews},
   pages = {435-440},
   publisher = {Genesis Global Publication},
   title = {ChatGPT: A Study of AI Language Processing and its Implications},
   volume = {04},
   year = {2023},
}

@misc{google1,
   title = {¿Qué es la IA generativa y cuáles son sus aplicaciones?  |  Google Cloud},
   url = {https://cloud.google.com/use-cases/generative-ai?hl=es},
}


@article{ETL,
   abstract = {Resumen-La tarea de un diseñador de procesos de ETL involucra: (1) analizar las fuentes de datos existentes para encontrar la semántica oculta en ellas y (2) diseñar el flujo de trabajo que extraiga los datos desde las fuentes, repare sus inconsistencias, los transforme en un formato deseado, y, finalmente, los inserte en la bodega de datos. Con el propósito de facilitar esta tarea, se han desarrollado diferentes técnicas, dos categorías que sobresalen son: (a) Las inspiradas en los diagramas de flujo y de procesos y (b) las inspiradas en el paradigma de programación orientada a objetos (POO) y los diagramas de UML. En el presente artículo se expone un par de alternativas halladas en la literatura y se ilustra la técnica utilizada en el proyecto "Desarrollo de una solución de inteligencia de negocios para apoyar a la toma de decisiones en el Proyecto Círculos de Aprendizaje", explicando el porqué de su elección y cómo se usó. Palabras clave-Bodegas de Datos, Inteligencia de Negocios, Zona de Preparación de Datos, Proceso de ETL. Abstract-The task of a designer ETL process involves: (1) analyzing existing data sources to find the semantics hidden in them and (2) design workflow that extracts data from sources, repair its inconsistencies, transforms it into a desired format, and finally inserted into the data warehouse. In order to facilitate this task, different techniques have been developed, two categories that stand out are: (a) inspired flow diagrams and process and (b) those based on the paradigm of object-oriented programming (OOP) and diagrams in UML. This article presents a couple of alternatives found in the literature and illustrates the technique used in the project "Development of a business intelligence solution to support decision making in the Learning Circles Project," explaining why of their choice and how it was used Key Word-Business Intelligence, Data Warehouse, Data Staging Area, ETL Process. I. INTRODUCCIÓN El proceso de extracción, transformación y carga-ETL (Extraction, Transformation and Load) es una de las actividades técnicas más críticas en el desarrollo de soluciones de inteligencia de negocios-BI (Business Intelligence) [1][2]. Hace parte del componente de integración y, de su implementación adecuada dependen la integridad, uniformidad, consistencia y disponibilidad de los datos utilizados en el componente de análisis de una solución de BI. Su función es extraer, limpiar, transformar, resumir, y formatear los datos que se almacenarán en la bodega de datos de la solución de BI [3][4][5]. La construcción del ETL puede dividirse en tres subprocesos o componentes: componente de extracción, componente de transformación y componente de carga. En la Tabla 1 se presenta la descripción de cada uno de estos componentes identificando los elementos objetivo, las operaciones realizadas, y los resultados esperados.},
   author = {Alexander Bustamante Martínez and Ernesto Amaru and Galvis Lista and Luis Carlos Gómez Flórez},
   issn = {0122-1701},
   issue = {1},
   journal = {Scientia et Technica Año XVIII},
   month = {3},
   title = {ETL Processes modeling techniques: an alternatives review and its application in a BI solution development project},
   volume = {18},
   year = {2013},
}

@misc{BuscadorAmbiental,
   title = {Buscador Ambiental},
   url = {https://www.buscadorambiental.cl/buscador/#/},
}

@misc{Ley20600,
   title = {Ley Chile - Ley 20600 - Biblioteca del Congreso Nacional},
   url = {https://www.bcn.cl/leychile/navegar?idNorma=1041361&idParte=9269911},
}

% ETL
@article{ETL1,
   abstract = {In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.},
   author = {Syed Muhammad Fawad Ali and Robert Wrembel},
   doi = {10.1007/S00778-017-0477-2/FIGURES/18},
   issn = {0949877X},
   issue = {6},
   journal = {VLDB Journal},
   keywords = {ETL conceptual design,ETL logical design,ETL optimization,ETL physical implementation,ETL workflow},
   month = {12},
   pages = {777-801},
   publisher = {Springer New York LLC},
   title = {From conceptual design to performance optimization of ETL workflows: current state of research and open problems},
   volume = {26},
   url = {https://link.springer.com/article/10.1007/s00778-017-0477-2},
   year = {2017},
}


@article{riego1,
   author = {Philip Feldman and James R. Foulds and Shimei Pan},
   isbn = {2306.06085v1},
   month = {6},
   title = {Trapping LLM Hallucinations Using Tagged Context Prompts},
   url = {https://arxiv.org/abs/2306.06085v1},
   year = {2023},
}


@article{langchain1,
   abstract = {ChatGPT is a type of language model that uses machine learning algorithms to generate responses to natural language input. Its advanced technology can revolutionize the way humans interact with machines by creating more natural and intuitive communication. However, the accuracy of ChatGPT's responses may be compromised by biased or inaccurate data, which highlights the importance of carefully evaluating its output. Furthermore, early versions of ChatGPT were found to produce academic papers with missing references, which may compromise the credibility of research in the academic publishing industry. This underscores the need to establish regulations and guidelines that ensure the ethical and transparent use of these technologies, and to avoid relying solely on automated tools like ChatGPT without thoroughly reviewing the literature. In order to minimise the potential for inaccurate information in the classroom, an intuitive application based on the AI language (OpenAI-also used by ChatGPT) has been implemented. The application has been designed in such a way that it does not use internet databases as a search source and the answers to the questions are based on pre-established documentation that has been checked/certified by the educational institution/professors. The developed application can be installed either locally on a personal computer or on a file server/online library etc. and does not require access to a reliable Internet connection as in the case of ChatGPT.},
   author = {Mirela Șorecău and Emil Șorecău},
   doi = {10.2478/kbo-2023-0084},
   keywords = {template},
   pages = {2023},
   title = {AN ALTERNATIVE APPLICATION TO CHATGPT THAT USES RELIABLE SOURCES TO ENHANCE THE LEARNING PROCESS},
   volume = {XXIX},
   year = {2023},
}

@article{raq,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG)-models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   journal = {Advances in Neural Information Processing Systems},
   pages = {9459-9474},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   volume = {33},
   url = {https://github.com/huggingface/transformers/blob/master/},
   year = {2020},
}

