@article{Benoit2018,
   abstract = {quanteda is an R package providing a comprehensive workflow and toolkit for natural language processing tasks such as corpus management, tokenization, analysis, and vi-sualization. It has extensive functions for applying dictionary analysis, exploring texts using keywords-in-context, computing document and feature similarities, and discovering multi-word expressions through collocation scoring. Based entirely on sparse operations, it provides highly efficient methods for compiling document-feature matrices and for manipulating these or using them in further quantitative analysis. Using C++ and multi-threading extensively, quanteda is also considerably faster and more efficient than other R and Python packages in processing large textual data. The package is designed for R users needing to apply natural language processing to texts, from documents to final analysis. Its capabilities match or exceed those provided in many end-user software applications, many of which are expensive and not open source. The package is therefore of great benefit to researchers, students, and other analysts with fewer financial resources. While using quanteda requires R programming knowledge, its API is designed to enable powerful, efficient analysis with a minimum of steps. By emphasizing consistent design, furthermore, quanteda lowers the barriers to learning and using NLP and quantitative text analysis even for proficient R programmers. Corpus management quanteda makes it easy to manage texts in the form of a "corpus", which is defined as a collection of texts that includes document-level variables specific to each text, as well as meta-data for documents and for the collection as a whole. With the package, users can easily segment texts by words, paragraphs, sentences, or even user-supplied delimiters and tags, group them into larger documents by document-level variables, or subset them based on logical conditions or combinations of document-level variables.},
   author = {Kenneth Benoit and Kohei Watanabe and Haiyan Wang and Paul Nulty and Adam Obeng and Stefan Müller and Akitaka Matsuo},
   doi = {10.21105/JOSS.00774},
   issue = {30},
   journal = {Journal of Open Source Software},
   month = {10},
   pages = {774},
   publisher = {The Open Journal},
   title = {quanteda: An R package for the quantitative analysis of textual data},
   volume = {3},
   year = {2018},
}
@article{Dunning1993,
   abstract = {Much work has been done on the statistical analysis of text. In some cases reported in the literature , inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results. This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text. However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical. This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.},
   author = {Ted Dunning},
   doi = {10.5555/972450.972454},
   title = {Accurate Methods for the Statistics of Surprise and Coincidence},
   year = {1993},
}
@article{Bormuth1969,
   abstract = {The studies reported here are part of a research program whose purpose is to increase the effectiveness with which students acquire knowledge from written instructional materials. The studies had both a basic and an applied objective. The basic objective was to obtain evidence upon which to base a theory of the processes involved in language comprehension. The correlations between a large number of linguistic features and a measure of the difficulty students exhibited in comprehending the written language samples in which those features occurred were determined. The number of linguistic features which can be conceptualized numbers in the hundreds, most of which must be regarded as potentially representing a stimulus involved in the comprehension processes, because present theory of comprehension is too primitive to permit the authors to identify or to rule out more than a few of those features. The applied objective was to develop regression formulas for estimating if instructional materials are suitable for students of varying levels of language comprehension ability. These readability formulas provide a partial solution to the problem of fitting materials to students. That is, students may be provided with materials suited to their levels of comprehension ability not only by manipulating the materials rto make them suitably understandable but also by selecting and using just those materials which are suited to the students' comprehension ability. (JL)},
   author = {John R Bormuth},
   title = {I S DOCUMENT COP YR 1GH TED? ERIC R EPRO DUCT( ON RELEASE? LEVEL OF AVA1 LABILITY YES 0 NO 0 YES 0 NO 0 TITLE Development of Readability Analysis DEVELOPMENT OF READABILITY ANALYSES},
   year = {1969},
}
@article{X1,
   abstract = {Collocation is a linguistic phenomenon that is difficult to define and harder to explain; it has been largely overlooked in the field of computational linguistics due to its difficulty. Although standard techniques exist for finding collocations, they tend to be rather noisy and suffer from sparse data problems. In this paper, we demonstrate that by utilising parsed input to concentrate on one very specific type of collocation-in this case, verbs with particles, a subset of the so-called "multi-word" verbs-and applying an algorithm to promote those colloca-tions in which we have more confidence, the problems with statistically learning collocations can be overcome.},
   author = {Don Blaheta and Mark Johnson},
   title = {Unsupervised learning of multi-word verbs *},
}
@article{X2,
   abstract = {Description Textual statistics functions formerly in the 'quanteda' package. Textual statistics for characterizing and comparing textual data. Includes functions for measuring term and document frequency, the co-occurrence of words, similarity and distance between features and documents, feature entropy, keyword occurrence, readability, and lexical diversity. These functions extend the 'quanteda' package and are specially designed for sparse textual data. License GPL-3 Depends R (>= 3.5.0) Imports quanteda, Matrix, methods, nsyllable, proxyC (>= 0.1.4), Rcpp (>= 0.12.12), RcppParallel, stringi LinkingTo Rcpp, RcppParallel, RcppArmadillo (>= 0.7.600.1.0), quanteda Suggests entropy, ExPosition, proxy, rmarkdown, spelling, svs, testthat, knitr URL https://quanteda.io Encoding UTF-8 BugReports https://github.com/quanteda/quanteda.textstats/issues LazyData TRUE Language en-GB RoxygenNote 7.1.2},
   title = {Package 'quanteda.textstats' Title Textual Statistics for the Quantitative Analysis of Textual Data},
   url = {https://orcid.org/0000-0003-4992-4311},
   year = {2021},
}
@article{Rong2016,
   abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model. 1 1 Continuous Bag-of-Word Model 1.1 One-word context We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model. For readers who are new to neural networks, it is recommended that one go through Appendix A for a quick review of the important concepts and terminologies before proceeding further. Figure 1 shows the network model under the simplified context definition 2. In our setting, the vocabulary size is V , and the hidden layer size is N. The units on adjacent 1 An online interactive demo is available at: http://bit.ly/wevi-online. 2 In Figures 1, 2, 3, and the rest of this note, W is not the transpose of W, but a different matrix instead.},
   author = {Xin Rong},
   title = {word2vec Parameter Learning Explained},
   url = {http://bit.ly/wevi-online.},
   year = {2016},
}
@article{Benoit2018,
   abstract = {Political scientists lack domain-specific measures for the purpose of measuring the sophistication of political communication. We systematically review the shortcomings of existing approaches, before developing  a new and better method  along with software tools to apply it.  We use  crowdsourcing to perform thousands of  pairwise comparisons of text snippets and  incorporate these results into a  statistical model of sophistication.  This  includes previously excluded features such as parts of speech, and a measure of  word rarity derived from dynamic term frequencies in the Google books dataset.  Our technique not only shows  which features are appropriate to the political  domain and how, but also provides a measure easily applied and re-scaled to  political texts in a  way that facilitates probabilistic comparisons. We  reanalyze the State of the Union corpus to demonstrate how  conclusions differ when using our improved approach, including the ability to compare complexity as a function of covariates.},
   author = {Kenneth Benoit and Kevin Munger and Arthur Spirling},
   doi = {10.2139/SSRN.3062061},
   journal = {SSRN Electronic Journal},
   keywords = {Arthur Spirling,Kenneth Benoit,Kevin Munger,Measuring and Explaining Political Sophistication Through Textual Complexity,SSRN,crowdsourcing,pairwise contests,political methodology,sophistication,text as data},
   month = {5},
   publisher = {Elsevier BV},
   title = {Measuring and Explaining Political Sophistication Through Textual Complexity},
   url = {https://papers.ssrn.com/abstract=3062061},
   year = {2018},
}
@misc{X3,
   title = {La caída del Silicon Valley Bank | Diario Financiero},
   url = {https://www.df.cl/internacional/economia/silicon-valley-bank-la-espectacular-caida-del-banquero-de-la-industria},
}
@misc{X4,
   title = {SVB Took the Wrong Risks - Bloomberg},
   url = {https://www.bloomberg.com/opinion/articles/2023-03-14/svb-took-the-wrong-risks?leadSource=uverify%20wall},
}
@misc{X5,
   title = {Silvergate anuncia el cierre y la liquidación de su banco cripto | Diario Financiero},
   url = {https://www.df.cl/mercados/banca-fintech/silvergate-anuncia-el-cierre-y-la-liquidacion-de-su-banco-cripto},
}
@article{Sandoz2008,
   abstract = {Public-private partnerships (PPP) are used by governments as a means to finance infrastructure services such as roads, bridges or hospitals. In egovernment, public services are provided and delivered to citizens and businesses through the internet. The end user should be able to measure a difference in terms of service quality and comfort, or necessary effort, delay, and cost, between acquiring the service over the net vs. using traditional channels. This paper presents a case implementation of PPP in e-government in Geneva, Switzerland. In this scheme, a private third party is introduced aside the public administration and the end-user in order to maximize the added value to the user. In comparison to the traditional client-server model of internet transaction, several new problems are encountered at the organizational, process and technical levels. The paper describes the solutions and underlying architectures. © 2008 IEEE.},
   author = {Alain Sandoz and Jean René Eudes and Raphaëlle Prévôt},
   doi = {10.1109/MCETECH.2008.14},
   isbn = {0769530826},
   journal = {Proceedings - 2008 International MCETECH Conference on e-Technologies, MCETECH 2008},
   pages = {203-211},
   title = {Public-private partnership in e-government: A case implementation},
   year = {2008},
}
@article{Palaco2019,
   abstract = {Although public–private partnerships (PPP) and electronic government (e-government) have proven to be fruitful mechanisms for economic development and emerging economies seem to recognize their importance, consistent methods for analyzing the early planning stages of e-government portfolios are lacking. The present work utilized a comprehensive literature review to understand the evaluation criteria for PPP projects throughout the early-stage planning process. A qualitative meta-synthesis was employed to identify critical factors for PPP and e-government, with a particular focus on developing countries, PPP, and e-government criteria. Our research presents a framework named “PPP4e-Gov” (public–private partnerships for e-government). The framework compares risk and value factors of e-government PPP projects and adopts a weighted scoring model that estimates the risks that should be considered in a project and how much value a given e-government initiative may generate if the PPP option is chosen. As an illustration of how the framework may be used, the paper interviewed ten practitioners in Costa Rica who tried out PPP4e-Gov and showcased how to plan their e-government initiatives.},
   author = {Ileana Palaco and Min Jae Park and Suk Kyoung Kim and Jae Jeung Rho},
   doi = {10.1016/j.evalprogplan.2018.10.015},
   issn = {01497189},
   journal = {Evaluation and Program Planning},
   keywords = {Costa Rica,Developing countries,Electronic government,Public–private partnerships},
   month = {2},
   pages = {205-218},
   pmid = {30415092},
   publisher = {Elsevier Ltd},
   title = {Public–private partnerships for e-government in developing countries: An early stage assessment framework},
   volume = {72},
   year = {2019},
}
@article{Palaco2019,
   abstract = {Although public–private partnerships (PPP) and electronic government (e-government) have proven to be fruitful mechanisms for economic development and emerging economies seem to recognize their importance, consistent methods for analyzing the early planning stages of e-government portfolios are lacking. The present work utilized a comprehensive literature review to understand the evaluation criteria for PPP projects throughout the early-stage planning process. A qualitative meta-synthesis was employed to identify critical factors for PPP and e-government, with a particular focus on developing countries, PPP, and e-government criteria. Our research presents a framework named “PPP4e-Gov” (public–private partnerships for e-government). The framework compares risk and value factors of e-government PPP projects and adopts a weighted scoring model that estimates the risks that should be considered in a project and how much value a given e-government initiative may generate if the PPP option is chosen. As an illustration of how the framework may be used, the paper interviewed ten practitioners in Costa Rica who tried out PPP4e-Gov and showcased how to plan their e-government initiatives.},
   author = {Ileana Palaco and Min Jae Park and Suk Kyoung Kim and Jae Jeung Rho},
   doi = {10.1016/j.evalprogplan.2018.10.015},
   issn = {01497189},
   journal = {Evaluation and Program Planning},
   keywords = {Costa Rica,Developing countries,Electronic government,Public–private partnerships},
   month = {2},
   pages = {205-218},
   pmid = {30415092},
   publisher = {Elsevier Ltd},
   title = {Public–private partnerships for e-government in developing countries: An early stage assessment framework},
   volume = {72},
   year = {2019},
}
@misc{X6,
   title = {Tribunales ambientales- Ley Fácil - es},
   url = {https://www.bcn.cl/portal/leyfacil/recurso/tribunales-ambientales},
}
@article{Chile2014,
   abstract = {La Biblioteca del Congreso Nacional de Chile (BCN) es un servicio común del Senado y la Cámara de Diputados, cuya misión es apoyar a la comunidad parlamentaria en el ejercicio de sus funciones constitucionales a través de la generación y provisión de productos y servicios de calidad, accesibles, oportunos, pertinentes y políticamente neutrales; asimismo, contribuir a la vinculación del Congreso Nacional de Chile con la ciudadanía, dando acceso a su acervo jurídico e histórico, y promoviendo instancias de diálogo y reflexión entre los parlamentarios y la sociedad civil.},
   author = {Biblioteca del Congreso Nacional de Chile},
   keywords = {Acerca de la Biblioteca,Formación Cívica,Historia Política Legislativa,Información territorial,LeyChile,Observatorio Parlamentario},
   publisher = {Biblioteca del Congreso Nacional de Chile},
   title = {Biblioteca del Congreso Nacional de Chile},
   url = {https://www.bcn.cl/portal/leyfacil/recurso/tribunales-ambientales},
   year = {2014},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
   isbn = {1706.03762v7},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {5999-6009},
   publisher = {Neural information processing systems foundation},
   title = {Attention Is All You Need},
   volume = {2017-December},
   url = {https://arxiv.org/abs/1706.03762v7},
   year = {2017},
}
@misc{X6,
   title = {View of Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast},
   url = {https://as-proceeding.com/index.php/icaens/article/view/1127/1062},
}
@misc{X7,
   title = {LangChain},
   url = {https://www.langchain.com/},
}
@article{Neelakantan2022,
   abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.},
   author = {Arvind Neelakantan and Tao Xu and Raul Puri and Alec Radford and Jesse Michael Han and Jerry Tworek and Qiming Yuan and Nikolas Tezak and Jong Wook Kim and Chris Hallacy and Johannes Heidecke and Pranav Shyam and Boris Power and Tyna Eloundou Nekoul and Girish Sastry and Gretchen Krueger and David Schnurr and Felipe Petroski Such and Kenny Hsu and Madeleine Thompson and Tabarak Khan and Toki Sherbakov and Joanne Jang and Peter Welinder and Lilian Weng},
   month = {1},
   title = {Text and Code Embeddings by Contrastive Pre-Training},
   url = {https://arxiv.org/abs/2201.10005v1},
   year = {2022},
}
@article{Nagwani2015,
   abstract = {Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the semantic similarity computation in summarization process. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data. In this paper, a novel framework based on MapReduce technology is proposed for summarizing large text collection. The proposed technique is designed using semantic similarity based clustering and topic modeling using Latent Dirichlet Allocation (LDA) for summarizing the large text collection over MapReduce framework. The summarization task is performed in four stages and provides a modular implementation of multiple documents summarization. The presented technique is evaluated in terms of scalability and various text summarization parameters namely, compression ratio, retention ratio, ROUGE and Pyramid score are also measured. The advantages of MapReduce framework are clearly visible from the experiments and it is also demonstrated that MapReduce provides a faster implementation of summarizing large text collections and is a powerful tool in Big Text Data analysis.},
   author = {N. K. Nagwani},
   doi = {10.1186/S40537-015-0020-5/FIGURES/15},
   issn = {21961115},
   issue = {1},
   journal = {Journal of Big Data},
   keywords = {Big Text Data analysis,Clustering based summarization,Semantic similarity,Summarizing large text,Text clustering},
   month = {12},
   pages = {1-18},
   publisher = {SpringerOpen},
   title = {Summarizing large text collection using topic modeling and clustering based on MapReduce framework},
   volume = {2},
   url = {https://link.springer.com/articles/10.1186/s40537-015-0020-5 https://link.springer.com/article/10.1186/s40537-015-0020-5},
   year = {2015},
}
@misc{X8,
   title = {Introducing text and code embeddings},
   url = {https://openai.com/blog/introducing-text-and-code-embeddings},
}
@article{Momodu2023,
   author = {Victor Momodu},
   doi = {10.2139/SSRN.4574992},
   journal = {SSRN Electronic Journal},
   keywords = {Automotive,Large Language Models,SSRN,Victor Momodu},
   month = {8},
   publisher = {Elsevier BV},
   title = {Using Local Large Language Models to Simplify Requirement Engineering Documents in the Automotive Industry},
   url = {https://papers.ssrn.com/abstract=4574992},
   year = {2023},
}
@article{Arefeen2023,
   abstract = {Question-answering (QA) is a significant application of Large Language Models (LLMs), shaping chatbot capabilities across healthcare, education, and customer service. However, widespread LLM integration presents a challenge for small businesses due to the high expenses of LLM API usage. Costs rise rapidly when domain-specific data (context) is used alongside queries for accurate domain-specific LLM responses. One option is to summarize the context by using LLMs and reduce the context. However, this can also filter out useful information that is necessary to answer some domain-specific queries. In this paper, we shift from human-oriented summarizers to AI model-friendly summaries. Our approach, LeanContext, efficiently extracts $k$ key sentences from the context that are closely aligned with the query. The choice of $k$ is neither static nor random; we introduce a reinforcement learning technique that dynamically determines $k$ based on the query and context. The rest of the less important sentences are reduced using a free open source text reduction method. We evaluate LeanContext against several recent query-aware and query-unaware context reduction approaches on prominent datasets (arxiv papers and BBC news articles). Despite cost reductions of $37.29\%$ to $67.81\%$, LeanContext's ROUGE-1 score decreases only by $1.41\%$ to $2.65\%$ compared to a baseline that retains the entire context (no summarization). Additionally, if free pretrained LLM-based summarizers are used to reduce context (into human consumable summaries), LeanContext can further modify the reduced context to enhance the accuracy (ROUGE-1 score) by $13.22\%$ to $24.61\%$.},
   author = {Md Adnan Arefeen and Biplob Debnath and Srimat Chakradhar},
   month = {9},
   title = {LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs},
   url = {https://arxiv.org/abs/2309.00841v1},
   year = {2023},
}
@article{Hillebrand2023,
   abstract = {Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.},
   author = {Lars Hillebrand and Armin Berger and Tobias Deußer and Tim Dilmaghani and Mohamed Khaled and Bernd Kliem and Rüdiger Loitz and Maren Pielka and David Leonhard and Christian Bauckhage and Rafet Sifa and Chris-Tian Bauckhage},
   doi = {10.1145/3573128.3609344},
   isbn = {9798400700279},
   keywords = {CCS CONCEPTS • Information systems → Recommender systems,Informa-tion extraction,Language models},
   month = {8},
   pages = {1-4},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models},
   url = {https://dl.acm.org/doi/10.1145/3573128.3609344},
   year = {2023},
}
@article{Momodu2023,
   author = {Victor Momodu},
   doi = {10.2139/SSRN.4574992},
   journal = {SSRN Electronic Journal},
   keywords = {Automotive,Large Language Models,SSRN,Victor Momodu},
   month = {8},
   publisher = {Elsevier BV},
   title = {Using Local Large Language Models to Simplify Requirement Engineering Documents in the Automotive Industry},
   url = {https://papers.ssrn.com/abstract=4574992},
   year = {2023},
}
@article{X9,
   abstract = {This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.},
   author = {Ondřej Plátek and Vojtěch Hudeček and Patricia Schmidtová and Mateusz Lango and Ondřej Dušek},
   month = {8},
   title = {Three Ways of Using Large Language Models to Evaluate Chat},
   url = {https://arxiv.org/abs/2308.06502v1},
   year = {2023},
}
@article{Neelakantan2022,
   abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.},
   author = {Arvind Neelakantan and Tao Xu and Raul Puri and Alec Radford and Jesse Michael Han and Jerry Tworek and Qiming Yuan and Nikolas Tezak and Jong Wook Kim and Chris Hallacy and Johannes Heidecke and Pranav Shyam and Boris Power and Tyna Eloundou Nekoul and Girish Sastry and Gretchen Krueger and David Schnurr and Felipe Petroski Such and Kenny Hsu and Madeleine Thompson and Tabarak Khan and Toki Sherbakov and Joanne Jang and Peter Welinder and Lilian Weng},
   month = {1},
   title = {Text and Code Embeddings by Contrastive Pre-Training},
   url = {https://arxiv.org/abs/2201.10005v1},
   year = {2022},
}

@article{ETL,
   abstract = {Resumen-La tarea de un diseñador de procesos de ETL involucra: (1) analizar las fuentes de datos existentes para encontrar la semántica oculta en ellas y (2) diseñar el flujo de trabajo que extraiga los datos desde las fuentes, repare sus inconsistencias, los transforme en un formato deseado, y, finalmente, los inserte en la bodega de datos. Con el propósito de facilitar esta tarea, se han desarrollado diferentes técnicas, dos categorías que sobresalen son: (a) Las inspiradas en los diagramas de flujo y de procesos y (b) las inspiradas en el paradigma de programación orientada a objetos (POO) y los diagramas de UML. En el presente artículo se expone un par de alternativas halladas en la literatura y se ilustra la técnica utilizada en el proyecto "Desarrollo de una solución de inteligencia de negocios para apoyar a la toma de decisiones en el Proyecto Círculos de Aprendizaje", explicando el porqué de su elección y cómo se usó. Palabras clave-Bodegas de Datos, Inteligencia de Negocios, Zona de Preparación de Datos, Proceso de ETL. Abstract-The task of a designer ETL process involves: (1) analyzing existing data sources to find the semantics hidden in them and (2) design workflow that extracts data from sources, repair its inconsistencies, transforms it into a desired format, and finally inserted into the data warehouse. In order to facilitate this task, different techniques have been developed, two categories that stand out are: (a) inspired flow diagrams and process and (b) those based on the paradigm of object-oriented programming (OOP) and diagrams in UML. This article presents a couple of alternatives found in the literature and illustrates the technique used in the project "Development of a business intelligence solution to support decision making in the Learning Circles Project," explaining why of their choice and how it was used Key Word-Business Intelligence, Data Warehouse, Data Staging Area, ETL Process. I. INTRODUCCIÓN El proceso de extracción, transformación y carga-ETL (Extraction, Transformation and Load) es una de las actividades técnicas más críticas en el desarrollo de soluciones de inteligencia de negocios-BI (Business Intelligence) [1][2]. Hace parte del componente de integración y, de su implementación adecuada dependen la integridad, uniformidad, consistencia y disponibilidad de los datos utilizados en el componente de análisis de una solución de BI. Su función es extraer, limpiar, transformar, resumir, y formatear los datos que se almacenarán en la bodega de datos de la solución de BI [3][4][5]. La construcción del ETL puede dividirse en tres subprocesos o componentes: componente de extracción, componente de transformación y componente de carga. En la Tabla 1 se presenta la descripción de cada uno de estos componentes identificando los elementos objetivo, las operaciones realizadas, y los resultados esperados.},
   author = {Alexander Bustamante Martínez and Ernesto Amaru and Galvis Lista and Luis Carlos Gómez Flórez},
   issn = {0122-1701},
   issue = {1},
   journal = {Scientia et Technica Año XVIII},
   month = {3},
   title = {ETL Processes modeling techniques: an alternatives review and its application in a BI solution development project},
   volume = {18},
   year = {2013},
}
@misc{Ley20600,
   title = {Ley Chile - Ley 20600 - Biblioteca del Congreso Nacional},
   url = {https://www.bcn.cl/leychile/navegar?idNorma=1041361&idParte=9269911},
}

@misc{BuscadorAmbiental,
   title = {Buscador Ambiental},
   url = {https://www.buscadorambiental.cl/buscador/#/},
}

