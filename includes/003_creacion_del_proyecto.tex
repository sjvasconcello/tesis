

\chapter{Creación del Proyecto}


Los Tribunales Ambientales son órganos jurisdiccionales especiales, sujetos a la 
superintendencia directiva, correccional y económica de la Corte Suprema, cuya función 
es resolver las controversias medioambientales de su competencia y ocuparse de los 
demás asuntos que la ley somete a su conocimiento \cite{Ley20600}. Estos tribunales generan una cantidad 
de jurisprudencia que puede ser encontrada en su portal de consulta llamado buscador ambiental \cite{BuscadorAmbiental}. 

El proyecto consiste en la generación de un chatbot en donde se pueda preguntar sobre 
la jurisprudencia de estos tribunales, aunque por razones de capacidad el chatbot se 
vea acotado solamente a las reclamaciones recibidas por el tribunal, con una estructura representada a \autoref{fig:estructura}.

Por lo que, este proyecto consiste en un proceso de extracción de datos 
desde el buscador ambiental, transformación de estos datos para su utilización, generación 
de vectores de estos datos para que puedan interaccionar con la aplicación, carga de estos 
en una base de datos, para que después la aplicación pueda interactuar con ellos y mandando 
esa información a el LLM, siendo en este caso gpt-4 perteneciente a OpenAI.

% Inclusión de Figuras
\begin{figure}[ht!]
    \centering
    \includegraphics[width=.5\textwidth]{figures/huemul1.png}
    \caption[Estructura basica de la aplicación]{Estructura basica de la aplicación\\
    {\scriptsize (Fuente: Elaboración propia)}}
    \label{fig:estructura}
\end{figure}
    

A partir de esta estructura mientras se avance en el desarrollo, se explicará parte por parte el proceso y con ello los riesgos de cada uno de ellos.



\section{ETL}


Para realizar el proyecto fue necesario realizar un proceso de ETL. El término ETL se refiere a las técnicas de "Extracción, 
Transformación y Carga" (Extract, Transform, Load), que constituyen un proceso clave para los datos necesarios para el proyecto. 
Este proceso implica la extracción de datos de fuentes heterogéneas, su transformación para ajustarse a las necesidades del 
negocio y su posterior carga en un destino que, por lo general, es un almacén de datos diseñado para el análisis y la generación 
de informes\cite{ETL1}. Siendo en este proyecto un estrutura como la \autoref{fig:etl1}.

% Inclusión de Figuras
\begin{figure}[ht!]
    \centering
    \includegraphics[width=.7\textwidth]{figures/huemulETL.png}
    \caption[Estructura del proceso de ETL para el Buscador Ambiental]{Estructura del proceso de ETL para el Buscador Ambiental\\
    {\scriptsize (Fuente: Elaboración propia)}}
    \label{fig:etl1}
\end{figure}
    

La fase de extracción implica la recolección de datos de múltiples fuentes, que pueden variar desde bases de datos 
estructuradas hasta información no estructurada en la web. La transformación se refiere al proceso de limpieza, conversión, 
y consolidación de estos datos en un formato adecuado para el análisis. Finalmente, la carga es el proceso de transferir 
los datos transformados al sistema de destino, donde se pueden almacenar y utilizar para la toma de decisiones estratégicas 
\cite{ETL1}.

\subsection{Extract}

%\autoref{fig:logoind}

La información requerida para el desarrollo del Chatbot se obtuvo del "Buscador ambiental" del Tribunal de Protección Ambiental 
de Chile a través de su sitio web\cite{BuscadorAmbiental}. Este portal aloja todos los documentos públicos disponibles para su consulta en cualquiera 
de los tres tribunales ambientales. Para acceder a la base de datos necesaria, se llevó a cabo la creación de 
un bot capaz de recopilar las entradas de este buscador de manera análoga a un usuario convencional.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.8\textwidth]{figures/huemul2.png}
    \caption[Screenshot del Buscador de la pagina del Primer Tribunal Ambiental]{Buscador de la pagina del Primer Tribunal Ambiental\\
    {\scriptsize (Fuente: Pagina del Primer Tribunal Ambiental)}}
    \label{fig:extract1}
\end{figure}
    
Para esta tarea, se empleó Selenium, una herramienta originalmente diseñada para generar pruebas, pero que, debido a la naturaleza 
reactiva y dinámica de los sitios web, así como a la detección de bots por parte de algunas páginas, resultó ser la elección 
más apropiada. Este bot, después de explorar todas las páginas del buscador ambiental, como se ilustra en la \autoref{fig:extract1},
logró recuperar cada uno de los enlaces individuales que conducen a las páginas específicas de cada caso, tal como se 
muestra en la \autoref{fig:extract2}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.8\textwidth]{figures/huemul2.png}
    \caption[Screenshot de una sola reclamación en la pagina web del buscador ambiental]{Screenshot de una sola reclamación en la pagina web del buscador ambiental\\
    {\scriptsize (Fuente: Pagina del Primer Tribunal Ambiental)}}
    \label{fig:extract2}
\end{figure}
    
Posteriormente, se contemplaba la posibilidad de obtener tanto los enlaces a cada documento en formato PDF como la información 
detallada de cada uno de estos documentos mediante la creación de un nuevo bot. Sin embargo, durante el proceso de desarrollo 
de este bot, se logró acceder a la API que permitía obtener directamente todos los datos mencionados anteriormente. Esto suprimió 
la necesidad de crear otro tipo de bot utilizando Selenium, ya que bastaba con realizar una solicitud a la mencionada API.

Para completar la fase de extracción de datos (Extract), una vez que se había obtenido toda la información mediante las solicitudes 
a la API, el último paso consistió en generar nuevas solicitudes con el objetivo de descargar todos los archivos PDF de cada una 
de las entradas. Estos archivos ahora están descargados y listos para la próxima etapa del proyecto, que implica la transformación 
de los datos con el fin de obtener la información necesaria para construir la base de datos a partir de los documentos.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/extract_diagram.png}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}



\subsection{Transform}

% CHAGO: Ojo lo de semejanza semantica
\par En la continuación en el proceso de ETL (Extracción, Transformación y Carga), los PDFs que previamente han sido descargados 
requieren ser sometidos a modificaciones con el objetivo de convertir la información que inicialmente se presenta en un 
estado ``sucio'' en datos ``limpios'' que puedan ser adecuadamente utilizados en el proyecto. Este proceso se denomina 
``transformación'', o ``transform,'' en inglés.

\par Entre los datos descargados, nos encontramos con un extenso número de PDFs que presentan dificultades significativas para su 
manipulación. Esto se debe a que el Tribunal Ambiental no sigue un formato estándar en la estructura de las reclamaciones 
presentadas. En consecuencia, cada uno de los textos posee un formato propio, lo que complica en gran medida la extracción 
eficiente de las diversas secciones contenidas en dichos textos. Sin embargo, gracias al funcionamiento del proceso de semejanza 
semántica, esta diversidad de formatos no representa un problema insuperable para el proyecto.

\par No obstante, surgen dificultades adicionales cuando se trata de las reclamaciones que son presentadas a los tribunales ambientales 
en formato digital o, en su defecto, en forma de fotocopias. Esto implica que no todos los documentos están habilitados para su 
procesamiento. En consecuencia, el primer paso en el proceso de transformación involucra la discriminación de qué PDFs son 
susceptibles de ser procesados y cuáles no. Para llevar a cabo esta tarea, se ha desarrollado un script capaz de detectar 
texto dentro de un archivo PDF. Si el texto es legible, se almacena; de lo contrario, se elimina.

\par Una vez separados los PDFs legibles y adecuados para el trabajo posterior, se procede con la transformación de estos documentos 
al formato TXT (texto plano). Esta etapa se lleva a cabo considerando la conveniencia de trabajar con archivos en formato de 
texto en comparación con los archivos en formato PDF puro, dado que el próximo método de transformación, que implica el uso de
 map-reduce en Langchain, requiere que los datos estén en formato de texto.

 \begin{figure}[ht!]
    \centering
    \includegraphics[width=.5\textwidth]{figures/huemulFOLDERS.png}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

\subsubsection{Map-Reduce}


\par El proceso de Map-Reduce es un modelo de programación diseñado para procesar grandes cantidades de datos de manera eficiente, 
escalable y distribuida a través de clústeres de servidores. En el contexto de un archivo PDF muy grande, por ejemplo, si se quisiera resumir 
el contenido o analizar la frecuencia de ciertas palabras, Map-Reduce podría ser utilizado para dividir la tarea en partes más pequeñas y manejables. 
Primero, la función de map tomaría el texto del PDF y lo dividiría en elementos más pequeños, como párrafos o líneas, 
asignando a cada uno un resumen intermedio \cite{mapreduce}. Luego, la función de reduce recogería todos los resumenes intermedios asociados con el documento 
y los combinaría para producir un resultado agregado, con un resumen de todo el documento. 

% \par Dentro del contexto del uso de Langchain, el proceso de map-reduce implica, en primer lugar, la aplicación de una separación en chunks y genera un resumen 
%a cada chunk del documento de manera individual, considerándolos de manera independiente 
%(la fase de "Map"). Posteriormente, todos los 
%nuevos documentos resumidos se combinan en un documento (la fase de "Reduce"). 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.9\textwidth]{figures/huemul_mapreduce.jpg}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

\par Sin embargo, es importante destacar que un archivo .txt puede contener un número de tokens demasiado elevado como para ser 
reducido de manera inmediata. En situaciones de este tipo, es necesario recurrir a un proceso de subdivisión que fragmente los 
textos en segmentos con un número de tokens inferior al límite impuesto por la API de OpenAI.
Cada archivo .txt puede ser dividido, 
resumido y exportado a un nuevo archivo .txt una vez que ha sido fragmentado previamente en segmentos.

\par Los documentos procesados son combinados utilizando otro proceso de Langchain para obtener un resultado final consolidado. 
Para concluir el proceso de transformación, los resúmenes generados después de haber pasado por el procedimiento de map-reduce 
se someten a un último paso antes de ser incorporados en la base de datos. Este paso implica la fusión de los resúmenes con la 
información obtenida a través de la información Extraida por Selenium previamente, presentada en formato de texto. Este proceso 
resulta en la creación de un único documento que engloba toda la información, al cual nos referiremos como ``documentos finales''. 
Con esto, se concluye la fase de transformación y se procede al último procedimiento, conocido como ``carga'' (Load), que consiste 
en el almacenamiento estos documentos finales en la base de datos.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/transfrom_diagram.png}
    \caption[Diagrama de secuencia para el proceso de transformación (Trasform) de datos]{Diagrama de secuencia para el proceso de transformación (Trasform) de datos\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

\subsection{Load}

\par Al culminar el proceso de Extracción, Transformación y Carga (ETL), resulta fundamental llevar a cabo la fase de carga, 
también conocida como ``load'' en inglés, en la cual se incorporan todos los documentos previamente descargados y transformados 
en una base de datos. Para este proyecto, en el cual se utiliza LangChain, resulta de vital importancia fragmentar los documentos 
en secciones más pequeñas, por lo que se deben dividir en chunks todos los documentos.

\par Esta necesidad surge debido a que los documentos deben ser sometidos a un proceso de Embeddings antes de ser 
introducidos en la base de datos. Esto se debe principalmente a que las funciones de Embeddings tienen un límite en la extensión 
de grupos de caracteres, conocidos como ``tokens'', que pueden ser procesados. En el contexto del modelo de Embeddings 
``text-embedding-ada-002'', este límite se establece en 8191 tokens \cite{openai1}, lo que constituye la longitud máxima de los fragmentos.

\subsubsection{Embeddings}



\par Por lo tanto, cuando se trabaja con documentos extensos, es imperativo dividirlos en fragmentos más pequeños antes de proceder 
con su incorporación. Según la información proporcionada en el Blog de OpenAI, los embeddings son ``representaciones numéricas 
de conceptos convertidos en secuencias numéricas, lo que facilita que las computadoras comprendan las relaciones entre los 
conceptos''\cite{openai1}. En términos sencillos, los embeddings son representaciones vectoriales de texto que permiten su comprensión por 
parte del Modelo de Lenguaje de Gran Tamaño (LLM). Dado que los LLM son redes neuronales, el proceso de Embedding resulta 
esencial para traducir el texto en números, que es el formato comprensible para esta red neuronal.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.5\textwidth]{figures/huemul4.png}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

\par Los embeddings resultantes se almacenan posteriormente en una base de datos vectorial denominada ChromaDB. Esta base de 
datos ha sido diseñada para ser compacta, escalable y eficiente, con el propósito de almacenar y recuperar vectores de manera 
efectiva. ChromaDB genera índices que permiten una recuperación rápida y eficiente de los embeddings en función de las 
consultas realizadas por los usuarios\cite{langchain1}.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/load_diagram.png}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

\newpage
\section{Chatbot}

   
La estructura del chatbot se desarrolló en su totalidad utilizando el lenguaje de programación Python. Esta elección se debió a la 
experiencia del equipo en Python, lo que facilitó tanto la creación del frontend como del backend de la aplicación.

Para la parte de la interacción del usuario (frontend), se empleó Python junto con el framework Flask para la presentación 
de contenido en pantalla, incluyendo tanto la estructura de HTML como las hojas de estilo CSS. Además, se aprovechó la potencia del 
framework Bootstrap para agilizar el proceso de maquetación.

En cuanto al backend, se implementó una API con el objetivo de facilitar la interacción entre el frontend y el backend. Para este 
propósito, se utilizó FastAPI, un framework que permite la creación rápida de APIs y que ofrece la ventaja de contar con Swagger 
para la prueba y generación automática de documentación.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.8\textwidth]{figures/finalhuemul.png}
    \caption[Diagrama de funcionamiento del Chatbot]{Diagrama de funcionamiento del Chatbot\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

El funcionamiento interno de la aplicación se basa en el framework Langchain para la interacción con el Modelo de Lenguaje Grande (LLM). 
LangChain es un framework poderoso que simplifica el desarrollo de aplicaciones utilizando modelos de lenguaje grandes (LLM). Proporciona 
una interfaz única y personalizable capaz de gestionar diferentes LLM, incluida la gestión rápida, el procesamiento, el aumento de datos, 
la orquestación de agentes, el almacenamiento y la evaluación. Este marco versátil permite a los desarrolladores integrar perfectamente 
los LLM con sus flujos de trabajo del mundo real y datos con el mínimo esfuerzo \cite{langchain1}.

Para la base de datos se utilizó ChromaDB. ChromaDB es una base de datos vectorial sin esquemas diseñada específicamente para 
su uso en aplicaciones de inteligencia artificial. Es liviano y muy potente, lo que permite el almacenamiento, la recuperación 
y la gestión eficiente de datos vectoriales (Embeddings), lo cual es esencial para las aplicaciones de chat de documentos 
basadas en LangChain y OpenAI \cite{langchain1}. Para que finalmente estas trabajaran en conjunto con los modelos de OpenAI, 
específicamente el modelo gpt-4 que actualmente es el más potente del mercado.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.4\textwidth]{figures/huemul6.png}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}


La función principal del chatbot es generar respuestas utilizando la técnica "Retriever-Augmented Generation" (RAG), 
que implica proporcionar contexto adicional en el prompt enviado a OpenAI. Mientras que una generación simple suele 
constar de instrucciones y una consulta, en RAG se agrega contexto dentro del prompt con el propósito de reducir la 
probabilidad de alucinaciones y mejorar la calidad de las respuestas. 


En cambio, mediante ``Retriever-Augmented Generation'' 
consiste al igual que el proceso anterior en entregar las instrucciones y una consulta, pero junto ello se agrega un contexto 
de este, cosa de que el modelo largo de lenguaje tenga menor capacidad de alucinar y generar una mejor respuesta. Podemos decir
que ``Retriever-Augmented Generatio'' (RAG) se refiere a un modelo de generación de lenguaje que se mejora por la capacidad 
de recuperar información de una base de datos, como un índice de vectores que representan los documentos del buscador ambiental, 
además de la memoria que ya posee. Este enfoque se utiliza para mejorar la generación de respuestas en tareas de procesamiento 
de lenguaje natural (NLP) que requieren conocimiento intensivo \cite{raq}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.4\textwidth]{figures/huemul5.png}
    \caption[]{\\
    {\scriptsize (Fuente: Elavoración propia)}}
    \label{fig:chatbot1}
\end{figure}

Finalmente, este chatbot envía el prompt con las instrucciones y el contexto obtenido mediante el RAG realizado con 
Langchain a el LLM, siendo en este caso gpt-4 de OpenAI, luego de pasar por una función de Embedding, cosa de que 
pueda ser leído por el modelo. Por lo que se realiza un request con la información a la API de OpenAI, para que se 
obtenga el output con la respuesta.
