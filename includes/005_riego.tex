
\chapter{Evaluación de Riesgos}

\section{Creación del Proyecto}

\subsection{Calidad de los datos}

Para cualquier tipo de trabajo, aplicación o estudio la calidad de los datos es en extremo importante esto debido a que, la calidad de los datos 
es crítica para un sistema de Machine Learning, porque los datos deficientes podrían causar problemas graves como predicciones erróneas o baja 
precisión de clasificación \cite{calidad1}. Ahora si hablamos de LLM que entra en el terreno del Deep learning, simplificamos los atributos de calidad de datos 
en los tres más importantes para el Deep Learning: la fidelidad, la variedad y la veracidad de un conjunto de datos \cite{calidad1}.

Como ejemplo dentro usando el proyecto que acompaña a este análisis de riesgo, se tuvo problemas en ciertas reclamaciones tratadas en el proceso 
de ETL, debido principalmente a que no existe un estándar para subir las reclamaciones en el buscador ambiental, existían pdf que eran legibles y otros 
que eran solo fotocopias, lo que imposibilitaba la extracción de información. Incluso dentro de la calidad de los datos, se puede extrapolar a la 
calidad de la metadata que era anexa en la base de datos.

\subsection{Sesgos} %sesgo2

Los Modelos de Lenguaje Grande (LLM), al ser entrenados con una masiva cantidad de datos, pueden manifestar sesgos debido a 
la procedencia de los datos utilizados en su entrenamiento. Estos sesgos pueden dar lugar a desafíos cuando se aplican en 
contextos distintos, ya que las respuestas generadas por el modelo pueden no ser adecuadas ni ajustarse a la realidad de 
esos nuevos escenarios.

Los modelos preentrenados con corupus generados por humanos contien sesgos sociales hacia ciertos grupos demograficos, estos
sesgos son preocupantes, debido a que pueden ser propagados o incluso amplificados en las tareas que estos modelos realizan \cite{sesgo2}.

Como ejemplo podemos citar el dicho por Bill Gates en su entrevista ``Can AI Save the World? Expert Insights with Bill Gates''  en donde menciono que: 
``Los sesgos en los modelos de IA pueden llevar a diagnósticos incorrectos, como se vio en el ejemplo donde Chat GPT diagnosticó erróneamente la 
tuberculosis como gripe debido a las bajas tasas de tuberculosis en los EE. UU.'' \cite{billgates1}. Con lo que podemos dimensional el efecto real de estos sesgos 
en lo correcto que puede llegar a ser una respuesta por parte de estos modelos.

\subsection{Elección correcta del modelo}
Actualmente la oferta de grandes modelos de lenguaje es muy amplia, desde los privados como: ChatGPT, PaLM, Bloom, etc. 
Como también modelos de código abierto como: Llama 2, OpenLLaMA, Falcon, Dolly, etc. \cite{modelo2} Con sus respectivas variantes, debido a que existen 
variables del modelo como por ejemplo Llama 2 que se puede encontrar en versión de 7, 13 y 70 billones de parámetros \cite{modelos3}. 

Elegir un modelo para trabajar es sumamente importante debido a que: la diversidad y calidad de los datos de preentrenamiento influyen 
sustancialmente en la capacidad del modelo de lenguaje para comprender y proporcionar respuestas precisas, que el tamaño puede tener una 
gran influencia en el rendimiento, que el soporte lingüistico podría ser crucial dependiendo la necesidad \cite{modelos1}.


\subsection{Costos Monetarios}
Los costos relacionados con la creación o el uso de un modelo LLM pueden aumentar de manera exponencial. Por lo tanto, es fundamental tener en 
cuenta los costos asociados al utilizar un servidor externo, así como el costo de operar un servidor local, incluyendo el consumo de energía eléctrica.
Por ejemplo, hay estudios en donde realizando un ajuste al modelo, fine tunning, es consumo de energía es comparable al de pequeñas ciudades y el dióxido 
de carbono emitido es equivalente a 500 veces la de un vuelo de ida y vuelta entre Nueva York y San Francisco \cite{ft1}. 


\subsection{Funciones de Embedding}

Las funciones de Embedding son específicas para cada modelo de lenguaje y no son intercambiables entre modelos. Esto se debe a que los embeddings son 
representaciones de alto nivel provenientes de los pesos y parámetros de cada modelo, por lo que están diseñadas para captar y almacenar las relaciones 
semánticas específicas de cada modelo \cite{microsoft1}. Por lo que si quieres usar un modelo es necesario contar con su función de Embedding adema de la 
apacidad de contar con la posibilidad de usar dicha función. 


\subsection{Uso de información privada}

Actualmente las empresas que entregan servicios de LLM son muy herméticos con la manera en que entrenan sus modelos, por lo tanto, no sabemos 
con qué información han sido entrenados la cual no necesariamente es solamente publica, además toda la información que preguntamos por ejemplo a 
OpenAI va a los servidores y sirve para reentrenar a los modelos.

Se ha probado que los ataques de reconstrucción de datos son posibles, se ha propuesto un ataque de reconstrucción dirigido de caja negra donde el 
adversario conoce parte de un ejemplo de entrenamiento (es decir, un indicio de texto) e infiere el resto (por ejemplo, un número de tarjeta de crédito), 
con lo que la posibilidad de extraer datos de entrenamiento de los modelos puede representar un riesgo serio para la privacidad \cite{privacidad1}. 

Actualmente OpenAI esta siendo demandada tanto por violar los derechos de autor \cite{privacidad3} como por robo sistemático \cite{privacidad2}, debido a que quienes 
demandas alegan que sus obras han sido usadas para entrenar a sus modelos de LLM, por lo que hasta que no tengamos total transparencia del proceso, 
a pesar de que existen servicios donde tus inputs supuestamente no son usados para reentrenar el modelo \cite{openai4}, es preferible ser cautelosos con la 
información que se manda a los LLM si estos no están corriendo de manera local.

\subsection{Volatilidad del Mercado}

Hasta la fecha actual, el 06 de noviembre de 2023, la creación de Chatbots utilizando el método RAG se perfilaba como 
una de las tendencias más destacadas en el mercado, siendo posiblemente una de las aplicaciones más prometedoras de los LLM. 
No obstante, en este mismo día, durante la OpenAI DevDay Keynote, se anunciaron novedades significativas, como la entrada en 
escena de los GPTs, que permite personalizar versiones de ChatGPT con instrucciones, conocimiento extra y cualquier otra combinación 
de habilidades \cite{openai2}. Además, se introdujeron otros modelos como gpt-4 turbo, un playground de desarrollo para la herramienta, 
text-to-speech (TTS), entre otros \cite{openai3}.

En este contexto, comprometerse con cualquier tecnología conlleva riesgos, especialmente en este período caracterizado por una 
volatilidad extrema y una inversión extremadamente agresiva en inteligencia artificial. La Inteligencia Artificial Generativa 
continúa evolucionando de manera acelerada, lo que la hace cada vez más disruptiva y más eficiente. Por lo tanto, la investigación 
y la implementación de soluciones de inteligencia artificial centradas en asistentes o chatbots representan un compromiso de alto 
riesgo en este entorno en constante cambio.

\section{Uso de la Aplicación}

\subsection{Entrega de contexto adecuado}

Los LLM a menudo presentan alucinaciones, por lo que es esencial reducir la frecuencia de este fenómeno. 
Para lograrlo, la provisión de contexto dentro del prompt no es simplemente precisa, sino que resulta 
absolutamente indispensable. De hecho, la entrega de contexto adecuado dentro del prompt ha demostrado 
ser una medida altamente efectiva para reducir las alucinaciones, logrando una disminución de hasta un 
99.88 porciento \cite{riego1}.

Por consiguiente, la correcta entrega de contexto dentro del prompt desempeña un papel fundamental en 
la generación de respuestas precisas a las consultas. Esto se debe a que, ya sea que el contexto 
proporcionado sea correcto, incorrecto o incluso irrelevante, el modelo de lenguaje lo utilizará como base 
para generar sus respuestas.

En el contexto del proyecto, la generación de respuestas se basa por completo en la entrega de contexto 
dentro del prompt, lo que a veces puede dar lugar a la transmisión de más información de la necesaria 
debido al funcionamiento del framework de Langchain. Esto puede llevar a situaciones en las que el modelo, 
influenciado por la información incorrecta o adicional proporcionada, genere respuestas que no reflejan 
un output con una respuesta en su totalidad correcta.

\subsection{Limitaciones de la similitud de cosenos}

La similitud del coseno, siendo este método más usado en modelos RAG para la extracción de contexto en la base de datos, 
como medida de similitud semántica en los embeddings, particularmente para palabras de alta frecuencia en tareas de 
procesamiento de lenguaje natural (NLP) como preguntas y respuestas (QA), recuperación de información (IR) y traducción 
automática (MT) presenta limitaciones en su uso, esto principalmente sucede debido a que la frecuencia de las palabras en los 
datos de entrenamiento afecta la geometría representacional de los embeddings contextualizados, siendo las palabras de baja 
frecuencia más concentradas geométricamente \cite{coseno}.

Por lo tanto, este problema se extrapola a que, en el momento de querer recuperar contexto pertinente de la base de datos, 
cuando se realiza el proceso de semejanza semántica entre el prompt y los vectores de la base de datos, este pueda recibir 
información no relacionada con el prompt, por lo que se enviar como contexto y puede dar oportunidad a alucinaciones.


\subsection{Alucinaciones}

El termino alucinación se refiere a la generación de textos o respuestas que exhiben corrección gramatical, fluidez y autenticidad,
pero se desvían de las entradas de fuente proporcionadas (fidelidad) o no se alinean con la precisión factual (factualidad) \cite{alucionacion1}.
Siendo en palabras más simples la entrega de información invetada por el modelo.

Dicho lo anterior, podemos decir que este es un gran factor de riesgo para el uso de una aplicación, porque a pesar de estar usando un 
sistema RAG, que dificulta la posibilidad de generar alucinaciones, sigue estando la posibilidad de que estas sucedan lo que puede 
entregar un output con información errónea y si es que no se revisa con criterio, se podría a llevar a cometer graves errores debido 
al uso de información que es directamente falsa.

\subsection{Aprendizaje por Refuerzo con Retroalimentación Humana (RLHF)}

Los modelos grandes de lenguaje suelen dar respuestas que a veces tanto política como moralmente no son correctas, por lo que 
las empresas tienen por objetivo alinear los valores humanos con los sistemas de aprendizaje automático y dirigir los algoritmos 
de aprendizaje hacia los objetivos e intereses de los humanos \cite{RLHF}. A esto se le llama Aprendizaje por Refuerzo con 
Retroalimentación Humana (RLHF), esto puede llegar a ser un problema si es que se busca realizar una aplicación en un usuario con una 
cultura diferente al proveedor del modelo o que si el usuario al no expresarse bien el modelo se confunda y no entregue una respuesta 
satisfactoria.
